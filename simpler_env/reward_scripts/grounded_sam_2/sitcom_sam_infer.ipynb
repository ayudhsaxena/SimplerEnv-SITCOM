{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nResults is a list of dict with the following structure:\\n[\\n    {\\n        'scores': tensor([0.7969, 0.6469, 0.6002, 0.4220], device='cuda:0'), \\n        'labels': ['car', 'tire', 'tire', 'tire'], \\n        'boxes': tensor([[  89.3244,  278.6940, 1710.3505,  851.5143],\\n                        [1392.4701,  554.4064, 1628.6133,  777.5872],\\n                        [ 436.1182,  621.8940,  676.5255,  851.6897],\\n                        [1236.0990,  688.3547, 1400.2427,  753.1256]], device='cuda:0')\\n    }\\n]\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import pycocotools.mask as mask_util\n",
    "from pathlib import Path\n",
    "from supervision.draw.color import ColorPalette\n",
    "from utils.supervision_utils import CUSTOM_COLOR_MAP\n",
    "from PIL import Image\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection \n",
    "\n",
    "\"\"\"\n",
    "Hyper parameters\n",
    "\"\"\"\n",
    "GROUNDING_MODEL = \"IDEA-Research/grounding-dino-tiny\"\n",
    "TEXT_PROMPT = \"car. tire.\"\n",
    "IMG_PATH = \"notebooks/images/truck.jpg\"\n",
    "SAM2_CHECKPOINT = \"./checkpoints/sam2_hiera_large.pt\"\n",
    "SAM2_MODEL_CONFIG = \"sam2_hiera_l.yaml\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "OUTPUT_DIR = Path(\"outputs/grounded_sam2_hf_model_demo\")\n",
    "DUMP_JSON_RESULTS = True\n",
    "\n",
    "# create output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# environment settings\n",
    "# use bfloat16\n",
    "torch.autocast(device_type=DEVICE, dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "if torch.cuda.get_device_properties(0).major >= 8:\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# build SAM2 image predictor\n",
    "sam2_checkpoint = SAM2_CHECKPOINT\n",
    "model_cfg = SAM2_MODEL_CONFIG\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=DEVICE)\n",
    "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
    "\n",
    "# build grounding dino from huggingface\n",
    "model_id = GROUNDING_MODEL\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "grounding_model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(DEVICE)\n",
    "\n",
    "\n",
    "# setup the input image and text prompt for SAM 2 and Grounding DINO\n",
    "# VERY important: text queries need to be lowercased + end with a dot\n",
    "text = TEXT_PROMPT\n",
    "img_path = IMG_PATH\n",
    "\n",
    "image = Image.open(img_path)\n",
    "\n",
    "sam2_predictor.set_image(np.array(image.convert(\"RGB\")))\n",
    "\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    outputs = grounding_model(**inputs)\n",
    "\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    inputs.input_ids,\n",
    "    box_threshold=0.4,\n",
    "    text_threshold=0.3,\n",
    "    target_sizes=[image.size[::-1]]\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Results is a list of dict with the following structure:\n",
    "[\n",
    "    {\n",
    "        'scores': tensor([0.7969, 0.6469, 0.6002, 0.4220], device='cuda:0'), \n",
    "        'labels': ['car', 'tire', 'tire', 'tire'], \n",
    "        'boxes': tensor([[  89.3244,  278.6940, 1710.3505,  851.5143],\n",
    "                        [1392.4701,  554.4064, 1628.6133,  777.5872],\n",
    "                        [ 436.1182,  621.8940,  676.5255,  851.6897],\n",
    "                        [1236.0990,  688.3547, 1400.2427,  753.1256]], device='cuda:0')\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the box prompt for SAM 2\n",
    "input_boxes = results[0][\"boxes\"].cpu().numpy()\n",
    "\n",
    "masks, scores, logits = sam2_predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=input_boxes,\n",
    "    multimask_output=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Post-process the output of the model to get the masks, scores, and logits for visualization\n",
    "\"\"\"\n",
    "# convert the shape to (n, H, W)\n",
    "if masks.ndim == 4:\n",
    "    masks = masks.squeeze(1)\n",
    "\n",
    "\n",
    "confidences = results[0][\"scores\"].cpu().numpy().tolist()\n",
    "class_names = results[0][\"labels\"]\n",
    "class_ids = np.array(list(range(len(class_names))))\n",
    "\n",
    "labels = [\n",
    "    f\"{class_name} {confidence:.2f}\"\n",
    "    for class_name, confidence\n",
    "    in zip(class_names, confidences)\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Visualize image with supervision useful API\n",
    "\"\"\"\n",
    "img = cv2.imread(img_path)\n",
    "detections = sv.Detections(\n",
    "    xyxy=input_boxes,  # (n, 4)\n",
    "    mask=masks.astype(bool),  # (n, h, w)\n",
    "    class_id=class_ids\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Note that if you want to use default color map,\n",
    "you can set color=ColorPalette.DEFAULT\n",
    "\"\"\"\n",
    "box_annotator = sv.BoxAnnotator(color=ColorPalette.from_hex(CUSTOM_COLOR_MAP))\n",
    "annotated_frame = box_annotator.annotate(scene=img.copy(), detections=detections)\n",
    "\n",
    "label_annotator = sv.LabelAnnotator(color=ColorPalette.from_hex(CUSTOM_COLOR_MAP))\n",
    "annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
    "cv2.imwrite(os.path.join(OUTPUT_DIR, \"groundingdino_annotated_image.jpg\"), annotated_frame)\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(color=ColorPalette.from_hex(CUSTOM_COLOR_MAP))\n",
    "annotated_frame = mask_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "cv2.imwrite(os.path.join(OUTPUT_DIR, \"grounded_sam2_annotated_image_with_mask.jpg\"), annotated_frame)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Dump the results in standard format and save as json files\n",
    "\"\"\"\n",
    "\n",
    "def single_mask_to_rle(mask):\n",
    "    rle = mask_util.encode(np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\"))[0]\n",
    "    rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
    "    return rle\n",
    "\n",
    "if DUMP_JSON_RESULTS:\n",
    "    # convert mask into rle format\n",
    "    mask_rles = [single_mask_to_rle(mask) for mask in masks]\n",
    "\n",
    "    input_boxes = input_boxes.tolist()\n",
    "    scores = scores.tolist()\n",
    "    # save the results in standard format\n",
    "    results = {\n",
    "        \"image_path\": img_path,\n",
    "        \"annotations\" : [\n",
    "            {\n",
    "                \"class_name\": class_name,\n",
    "                \"bbox\": box,\n",
    "                \"segmentation\": mask_rle,\n",
    "                \"score\": score,\n",
    "            }\n",
    "            for class_name, box, mask_rle, score in zip(class_names, input_boxes, mask_rles, scores)\n",
    "        ],\n",
    "        \"box_format\": \"xyxy\",\n",
    "        \"img_width\": image.width,\n",
    "        \"img_height\": image.height,\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(OUTPUT_DIR, \"grounded_sam2_hf_model_demo_results.json\"), \"w\") as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 519, 829)\n",
      "Full visualization saved to outputs/object_distance/distance_test_img.png\n",
      "Centroids-only visualization saved to outputs/object_distance/test_img_centroids_only.jpg\n",
      "Distance between carrot and gripper: 334.34 pixels\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import pycocotools.mask as mask_util\n",
    "from pathlib import Path\n",
    "from supervision.draw.color import ColorPalette\n",
    "from utils.supervision_utils import CUSTOM_COLOR_MAP\n",
    "from PIL import Image\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection \n",
    "\n",
    "class GroundedSAM2:\n",
    "    \"\"\"\n",
    "    A class that integrates Grounding DINO for object detection and SAM2 for segmentation.\n",
    "    Provides functionality to calculate distances between objects in an image.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        sam2_checkpoint=\"./checkpoints/sam2_hiera_large.pt\", \n",
    "        sam2_model_config=\"sam2_hiera_l.yaml\",\n",
    "        grounding_model=\"IDEA-Research/grounding-dino-tiny\",\n",
    "        device=None,\n",
    "        box_threshold=0.4,\n",
    "        text_threshold=0.3,\n",
    "        output_dir=\"outputs/grounded_sam2_results\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the GroundedSAM2 model.\n",
    "        \n",
    "        Args:\n",
    "            sam2_checkpoint (str): Path to the SAM2 model checkpoint.\n",
    "            sam2_model_config (str): SAM2 model configuration file.\n",
    "            grounding_model (str): Hugging Face model ID for Grounding DINO.\n",
    "            device (str, optional): Device to run the model on. If None, will use CUDA if available.\n",
    "            box_threshold (float): Confidence threshold for bounding boxes.\n",
    "            text_threshold (float): Confidence threshold for text prompts.\n",
    "            output_dir (str): Directory to save results.\n",
    "        \"\"\"\n",
    "        # Initialize device\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Set thresholds\n",
    "        self.box_threshold = box_threshold\n",
    "        self.text_threshold = text_threshold\n",
    "        \n",
    "        # Setup output directory\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Setup environment\n",
    "        self._setup_environment()\n",
    "        \n",
    "        # Build SAM2 image predictor\n",
    "        self.sam2_model = build_sam2(sam2_model_config, sam2_checkpoint, device=self.device)\n",
    "        self.sam2_predictor = SAM2ImagePredictor(self.sam2_model)\n",
    "        \n",
    "        # Build grounding dino from huggingface\n",
    "        self.processor = AutoProcessor.from_pretrained(grounding_model)\n",
    "        self.grounding_model = AutoModelForZeroShotObjectDetection.from_pretrained(grounding_model).to(self.device)\n",
    "    \n",
    "    def _setup_environment(self):\n",
    "        \"\"\"Set up the environment for the model.\"\"\"\n",
    "        # Use bfloat16\n",
    "        torch.autocast(device_type=self.device, dtype=torch.bfloat16).__enter__()\n",
    "        \n",
    "        if self.device == \"cuda\" and torch.cuda.get_device_properties(0).major >= 8:\n",
    "            # Turn on tfloat32 for Ampere GPUs\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    def _format_text_prompts(self, *objects):\n",
    "        \"\"\"\n",
    "        Format text prompts for Grounding DINO.\n",
    "        \n",
    "        Args:\n",
    "            *objects: Object names to detect.\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted text prompt string.\n",
    "        \"\"\"\n",
    "        # VERY important: text queries need to be lowercased + end with a dot\n",
    "        formatted_prompts = []\n",
    "        for obj in objects:\n",
    "            if not obj.endswith('.'):\n",
    "                obj += '.'\n",
    "            formatted_prompts.append(obj.lower())\n",
    "        \n",
    "        return ' '.join(formatted_prompts)\n",
    "    \n",
    "    def process_image(self, img_path, *object_names):\n",
    "        \"\"\"\n",
    "        Process an image to detect and segment objects.\n",
    "        \n",
    "        Args:\n",
    "            img_path (str): Path to the input image.\n",
    "            *object_names: Names of objects to detect.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image, detections, class_names)\n",
    "                - image: The original image as a numpy array\n",
    "                - detections: Supervision Detections object with masks and bounding boxes\n",
    "                - class_names: List of class names detected\n",
    "        \"\"\"\n",
    "        # Format text prompts\n",
    "        text_prompt = self._format_text_prompts(*object_names)\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path)\n",
    "        img_np = np.array(image.convert(\"RGB\"))\n",
    "        \n",
    "        # Set image for SAM2\n",
    "        self.sam2_predictor.set_image(img_np)\n",
    "        \n",
    "        # Process with Grounding DINO\n",
    "        inputs = self.processor(images=img_np, text=text_prompt, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.grounding_model(**inputs)\n",
    "        \n",
    "        results = self.processor.post_process_grounded_object_detection(\n",
    "            outputs,\n",
    "            inputs.input_ids,\n",
    "            box_threshold=self.box_threshold,\n",
    "            text_threshold=self.text_threshold,\n",
    "            target_sizes=[image.size[::-1]]\n",
    "        )\n",
    "        \n",
    "        # Get bounding boxes from grounding model\n",
    "        input_boxes = results[0][\"boxes\"].cpu().numpy()\n",
    "        \n",
    "        # Get masks from SAM2\n",
    "        masks, scores, logits = self.sam2_predictor.predict(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            box=input_boxes,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        \n",
    "        print(masks.shape)\n",
    "        # Ensure masks have correct shape (n, H, W)\n",
    "        if masks.ndim == 4:\n",
    "            masks = masks.squeeze(1)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Get class information\n",
    "        confidences = results[0][\"scores\"].cpu().numpy().tolist()\n",
    "        class_names = results[0][\"labels\"]\n",
    "        class_ids = np.array(list(range(len(class_names))))\n",
    "        \n",
    "        # Create detections object\n",
    "        detections = sv.Detections(\n",
    "            xyxy=input_boxes,\n",
    "            mask=masks.astype(bool),\n",
    "            class_id=class_ids,\n",
    "            confidence=np.array(confidences)\n",
    "        )\n",
    "        \n",
    "        return img_np, detections, class_names\n",
    "    \n",
    "    def _find_object_mask_index(self, class_names, detections, target_object):\n",
    "        \"\"\"\n",
    "        Find the index of the most confident mask for a target object.\n",
    "        \n",
    "        Args:\n",
    "            class_names (list): List of class names.\n",
    "            detections (sv.Detections): Detections object containing masks.\n",
    "            target_object (str): Target object name to find.\n",
    "            \n",
    "        Returns:\n",
    "            int: Index of the most confident mask for the target object, or -1 if not found.\n",
    "        \"\"\"\n",
    "        target_object = target_object.lower().strip('.')\n",
    "        \n",
    "        # Find all instances of the target object\n",
    "        indices = [i for i, name in enumerate(class_names) if name.lower() == target_object]\n",
    "        \n",
    "        if not indices:\n",
    "            return -1\n",
    "        \n",
    "        # Find the most confident detection\n",
    "        confidences = detections.confidence[indices]\n",
    "        if len(confidences) == 0:\n",
    "            return -1\n",
    "            \n",
    "        most_confident_idx = indices[np.argmax(confidences)]\n",
    "        return most_confident_idx\n",
    "    \n",
    "    def _calculate_mask_centroid(self, mask):\n",
    "        \"\"\"\n",
    "        Calculate the centroid of a binary mask.\n",
    "        \n",
    "        Args:\n",
    "            mask (np.ndarray): Binary mask array.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (x, y) coordinates of the centroid.\n",
    "        \"\"\"\n",
    "        # Find all points where the mask is True\n",
    "        y_indices, x_indices = np.where(mask)\n",
    "        \n",
    "        # Calculate centroid\n",
    "        if len(x_indices) > 0 and len(y_indices) > 0:\n",
    "            centroid_x = np.mean(x_indices)\n",
    "            centroid_y = np.mean(y_indices)\n",
    "            return (centroid_x, centroid_y)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def calculate_object_distance(self, img_path, object1, object2, visualize=False):\n",
    "        \"\"\"\n",
    "        Calculate the distance between centroids of two objects in an image.\n",
    "        \n",
    "        Args:\n",
    "            img_path (str): Path to the input image.\n",
    "            object1 (str): Name of the first object.\n",
    "            object2 (str): Name of the second object.\n",
    "            visualize (bool, optional): Whether to save visualization of results.\n",
    "            \n",
    "        Returns:\n",
    "            float: Distance between the centroids in pixels, or -1 if either object is not found.\n",
    "        \"\"\"\n",
    "        # Process the image\n",
    "        img, detections, class_names = self.process_image(img_path, object1, object2)\n",
    "        \n",
    "        # Find the most confident mask for each object\n",
    "        idx1 = self._find_object_mask_index(class_names, detections, object1)\n",
    "        idx2 = self._find_object_mask_index(class_names, detections, object2)\n",
    "        \n",
    "        if idx1 == -1 or idx2 == -1:\n",
    "            print(f\"Error: Could not find {'both' if idx1 == -1 and idx2 == -1 else object1 if idx1 == -1 else object2} in the image.\")\n",
    "            return -1\n",
    "        \n",
    "        # Get the masks\n",
    "        mask1 = detections.mask[idx1]\n",
    "        mask2 = detections.mask[idx2]\n",
    "        \n",
    "        # Calculate centroids\n",
    "        centroid1 = self._calculate_mask_centroid(mask1)\n",
    "        centroid2 = self._calculate_mask_centroid(mask2)\n",
    "        \n",
    "        if centroid1 is None or centroid2 is None:\n",
    "            print(\"Error: Could not calculate centroids for one or both objects.\")\n",
    "            return -1\n",
    "        \n",
    "        # Calculate Euclidean distance\n",
    "        distance = np.sqrt((centroid1[0] - centroid2[0])**2 + (centroid1[1] - centroid2[1])**2)\n",
    "        \n",
    "        # Visualize if requested\n",
    "        if visualize:\n",
    "            self._visualize_results(img_path, img, detections, class_names, centroid1, centroid2, distance)\n",
    "        \n",
    "        return distance\n",
    "    \n",
    "    def _visualize_results(self, img_path, img, detections, class_names, centroid1, centroid2, distance):\n",
    "        \"\"\"\n",
    "        Visualize the results and save the output images.\n",
    "        \n",
    "        Args:\n",
    "            img_path (str): Path to the input image.\n",
    "            img (np.ndarray): Image as a numpy array.\n",
    "            detections (sv.Detections): Detections object with masks and bounding boxes.\n",
    "            class_names (list): List of class names.\n",
    "            centroid1 (tuple): (x, y) coordinates of first centroid.\n",
    "            centroid2 (tuple): (x, y) coordinates of second centroid.\n",
    "            distance (float): Distance between centroids.\n",
    "        \"\"\"\n",
    "        # Create a copy of the image for visualization\n",
    "        img_vis = img.copy()\n",
    "        img_vis = cv2.cvtColor(img_vis, cv2.COLOR_RGB2BGR)  # Convert to BGR for OpenCV\n",
    "        \n",
    "        # Prepare labels\n",
    "        labels = [\n",
    "            f\"{class_name} {confidence:.2f}\"\n",
    "            for class_name, confidence\n",
    "            in zip(class_names, detections.confidence)\n",
    "        ]\n",
    "        \n",
    "        # Draw bounding boxes and labels\n",
    "        box_annotator = sv.BoxAnnotator(color=ColorPalette.from_hex(CUSTOM_COLOR_MAP))\n",
    "        img_vis = box_annotator.annotate(scene=img_vis, detections=detections)\n",
    "        \n",
    "        label_annotator = sv.LabelAnnotator(color=ColorPalette.from_hex(CUSTOM_COLOR_MAP))\n",
    "        img_vis = label_annotator.annotate(scene=img_vis, detections=detections, labels=labels)\n",
    "        \n",
    "        # Draw masks\n",
    "        mask_annotator = sv.MaskAnnotator(color=ColorPalette.from_hex(CUSTOM_COLOR_MAP))\n",
    "        img_vis = mask_annotator.annotate(scene=img_vis, detections=detections)\n",
    "        \n",
    "        # Draw centroids and line\n",
    "        centroid1 = (int(centroid1[0]), int(centroid1[1]))\n",
    "        centroid2 = (int(centroid2[0]), int(centroid2[1]))\n",
    "        \n",
    "        # Draw centroids as circles\n",
    "        cv2.circle(img_vis, centroid1, 5, (0, 0, 255), -1)  # Red\n",
    "        cv2.circle(img_vis, centroid2, 5, (0, 0, 255), -1)  # Red\n",
    "        \n",
    "        # Draw line between centroids\n",
    "        cv2.line(img_vis, centroid1, centroid2, (0, 255, 0), 2)  # Green\n",
    "        \n",
    "        # Put distance text\n",
    "        midpoint = ((centroid1[0] + centroid2[0]) // 2, (centroid1[1] + centroid2[1]) // 2)\n",
    "        cv2.putText(img_vis, f\"Distance: {distance:.2f} px\", midpoint, cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "        # Save the full visualization\n",
    "        output_file = os.path.join(self.output_dir, f\"distance_{os.path.basename(img_path)}\")\n",
    "        cv2.imwrite(output_file, img_vis)\n",
    "        print(f\"Full visualization saved to {output_file}\")\n",
    "        \n",
    "        # Create a second visualization showing just the two objects used for centroid calculation\n",
    "        img_objects_only = np.zeros_like(img)\n",
    "        base_filename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        \n",
    "        # Find the indices used for calculating centroids\n",
    "        idx1 = self._find_object_mask_index(class_names, detections, class_names[0])\n",
    "        idx2 = self._find_object_mask_index(class_names, detections, class_names[1])\n",
    "        \n",
    "        if idx1 != -1 and idx2 != -1:\n",
    "            # Create a filtered detections object with only the two objects of interest\n",
    "            filtered_indices = [idx1, idx2]\n",
    "            filtered_detections = sv.Detections(\n",
    "                xyxy=detections.xyxy[filtered_indices],\n",
    "                mask=detections.mask[filtered_indices],\n",
    "                class_id=detections.class_id[filtered_indices],\n",
    "                confidence=detections.confidence[filtered_indices],\n",
    "            )\n",
    "            \n",
    "            filtered_labels = [labels[i] for i in filtered_indices]\n",
    "            \n",
    "            # Create a visualization with just these two objects\n",
    "            img_objects_only = cv2.cvtColor(img.copy(), cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Draw the filtered objects\n",
    "            box_annotator = sv.BoxAnnotator(color=ColorPalette.from_hex(CUSTOM_COLOR_MAP))\n",
    "            img_objects_only = box_annotator.annotate(scene=img_objects_only, detections=filtered_detections)\n",
    "            \n",
    "            label_annotator = sv.LabelAnnotator(color=ColorPalette.from_hex(CUSTOM_COLOR_MAP))\n",
    "            img_objects_only = label_annotator.annotate(scene=img_objects_only, detections=filtered_detections, labels=filtered_labels)\n",
    "            \n",
    "            mask_annotator = sv.MaskAnnotator(color=ColorPalette.from_hex(CUSTOM_COLOR_MAP))\n",
    "            img_objects_only = mask_annotator.annotate(scene=img_objects_only, detections=filtered_detections)\n",
    "            \n",
    "            # Draw centroids\n",
    "            cv2.circle(img_objects_only, centroid1, 5, (0, 0, 255), -1)  # Red\n",
    "            cv2.circle(img_objects_only, centroid2, 5, (0, 0, 255), -1)  # Red\n",
    "            \n",
    "            # Draw line between centroids\n",
    "            cv2.line(img_objects_only, centroid1, centroid2, (0, 255, 0), 2)  # Green\n",
    "            \n",
    "            # Put distance text\n",
    "            cv2.putText(img_objects_only, f\"Distance: {distance:.2f} px\", midpoint, \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            \n",
    "            # Save the objects-only visualization\n",
    "            objects_output_file = os.path.join(self.output_dir, f\"{base_filename}_centroids_only.jpg\")\n",
    "            cv2.imwrite(objects_output_file, img_objects_only)\n",
    "            print(f\"Centroids-only visualization saved to {objects_output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "grounded_sam2 = GroundedSAM2(\n",
    "    sam2_checkpoint=\"./checkpoints/sam2_hiera_large.pt\",\n",
    "    sam2_model_config=\"sam2_hiera_l.yaml\",\n",
    "    grounding_model=\"IDEA-Research/grounding-dino-tiny\",\n",
    "    output_dir=\"outputs/object_distance\"\n",
    ")\n",
    "\n",
    "# Calculate distance between two objects\n",
    "img_path = \"notebooks/images/test_img.png\"\n",
    "object1 = \"carrot\"\n",
    "object2 = \"gripper\"\n",
    "\n",
    "distance = grounded_sam2.calculate_object_distance(img_path, object1, object2, visualize=True)\n",
    "\n",
    "if distance > 0:\n",
    "    print(f\"Distance between {object1} and {object2}: {distance:.2f} pixels\")\n",
    "else:\n",
    "    print(f\"Failed to calculate distance between {object1} and {object2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simpler_env_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
