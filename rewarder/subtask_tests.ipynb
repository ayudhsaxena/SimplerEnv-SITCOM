{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c10c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"./\")\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import base64\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cosine\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from typing import List, Dict, Any, Tuple, Iterator\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error\n",
    "from memory.reward_memory import RewardMemory\n",
    "from google.genai import errors\n",
    "import time\n",
    "import shutil  # For copying files\n",
    "import argparse  # For command line arguments\n",
    "\n",
    "min_reward = -2\n",
    "max_reward = 12\n",
    "bin_edges = np.linspace(min_reward, max_reward, num=10)\n",
    "\n",
    "\n",
    "class GeneralRewarder:\n",
    "    def __init__(self, reward_memory: RewardMemory, api_key: str = None, num_examples: int = 4, debug_mode: bool = False, debug_dir: str = \"debug_output\"):\n",
    "        \"\"\"\n",
    "        Initialize the GeneralRewarder with a RewardMemory.\n",
    "        \n",
    "        Args:\n",
    "            reward_memory: RewardMemory instance with processed data\n",
    "            api_key (str, optional): Google AI API key. If None, looks for environment variable\n",
    "            num_examples (int): Number of examples to use for in-context learning\n",
    "            debug_mode (bool): Whether to save debug information\n",
    "            debug_dir (str): Directory to save debug information\n",
    "        \"\"\"\n",
    "        # Configure the API key\n",
    "        if api_key is None:\n",
    "            api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "        \n",
    "        if not api_key:\n",
    "            raise ValueError(\"API key must be provided either as an argument or as GOOGLE_API_KEY environment variable\")\n",
    "        \n",
    "        self.client = genai.Client(api_key=api_key)\n",
    "        self.reward_memory = reward_memory\n",
    "        self.num_examples = num_examples\n",
    "        \n",
    "        # Debug mode settings\n",
    "        self.debug_mode = debug_mode\n",
    "        self.debug_dir = debug_dir\n",
    "        if self.debug_mode and not os.path.exists(self.debug_dir):\n",
    "            os.makedirs(self.debug_dir)\n",
    "        \n",
    "        # Set up model configuration for structured output\n",
    "        self.generation_config = types.GenerateContentConfig(\n",
    "            temperature=0.3,\n",
    "        )\n",
    "    \n",
    "    def encode_image(self, image_path: str) -> bytes:\n",
    "        \"\"\"Read an image file and return its bytes.\"\"\"\n",
    "        with open(image_path, \"rb\") as img_file:\n",
    "            return img_file.read()\n",
    "    \n",
    "    def create_system_instruction(self, subtask: str) -> str:\n",
    "        \"\"\"Create the system instruction with principles generation.\"\"\"\n",
    "        return (\n",
    "            f\"You are a helpful reward model. Your task is to evaluate how well is a transition from image1 to image2 \"\n",
    "            f\" to accomplish a subtask of the bigger task:\\n\"\n",
    "            f\"**Task:** Put carrot on plate in scene.\\n\\n\"\n",
    "            f\"**Subtask:** {subtask}\\n\\n\"\n",
    "            f\"First, establish a set of clear principles for evaluating the task.\\n\"\n",
    "            f\"When developing principles, focus on aspects such as:\\n\"\n",
    "            f\"- Proximity: How close is the gripper to the target object?\\n\"\n",
    "            f\"- Alignment: Is the gripper correctly oriented relative to the object?\\n\"\n",
    "            f\"- Gripper state: Is the gripper appropriately opened/closed for the task stage?\\n\"\n",
    "            f\"- Contact: Has the gripper made appropriate contact with the object?\\n\"\n",
    "            f\"- Object movement: Has the object been moved in the intended direction?\\n\\n\"\n",
    "            f\"Then, apply these principles to evaluate the transition with:\\n\"\n",
    "            f\"- A <principles> section outlining your evaluation framework\\n\"\n",
    "            f\"- A <reason> explaining your specific evaluation of this transition\\n\"\n",
    "            f\"- A scalar <reward> (integer)\\n\"\n",
    "        )\n",
    "    \n",
    "    def create_conversation(self, examples: List[Dict[str, Any]], system_instruction: str, \n",
    "                            test_image1_path: str, test_image2_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Create the conversation for the model.\"\"\"\n",
    "        conversation = []\n",
    "\n",
    "        # Add the examples for few-shot learning\n",
    "        for idx, ex in enumerate(examples):\n",
    "            # Create user message with instructions and first image\n",
    "            user_message = types.Content(\n",
    "                role=\"user\",\n",
    "                parts=[\n",
    "                    types.Part.from_text(\n",
    "                        text=system_instruction if idx == 0 else \"Evaluate the following transition.\\n\\n\"\n",
    "                        \"The first image is <image1> (initial), the second is <image2> (result).\\n\"\n",
    "                        \"Return <principles>, <reason>, and a single integer <reward> tag.\"\n",
    "                    ),\n",
    "                    types.Part.from_bytes(\n",
    "                        data=self.encode_image(ex['image1_path']),\n",
    "                        mime_type=\"image/jpeg\"\n",
    "                    ),\n",
    "                    types.Part.from_bytes(\n",
    "                        data=self.encode_image(ex['image2_path']),\n",
    "                        mime_type=\"image/jpeg\"\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            conversation.append(user_message)\n",
    "            \n",
    "            # For reward class, convert to integer string\n",
    "            # reward = str(ex['reward_class'])\n",
    "            raw_reward = ex[\"raw_reward\"]\n",
    "            global bin_edges\n",
    "            reward_class = 0\n",
    "            for i in range(len(bin_edges) - 1):\n",
    "                if bin_edges[i] <= raw_reward <= bin_edges[i + 1]:\n",
    "                    reward_class = i\n",
    "                    break\n",
    "            reward = str(reward_class)\n",
    "            # Generate a placeholder reason if not available\n",
    "            reason = f\"This transition shows progress towards the goal of {ex.get('original_subtask', 'the subtask')}.\"\n",
    "            \n",
    "            # Generate a placeholder principles section\n",
    "            principles = (\n",
    "                \"1. Proximity: Evaluate how close the gripper gets to the target object\\n\"\n",
    "                \"2. Alignment: Assess if the gripper is correctly oriented\\n\"\n",
    "                \"3. Contact: Determine if appropriate contact is made with the object\\n\"\n",
    "                \"4. Movement: Analyze if the object moves in the desired direction\"\n",
    "            )\n",
    "            \n",
    "            # Create model response\n",
    "            model_message = types.Content(\n",
    "                role=\"model\",\n",
    "                parts=[types.Part.from_text(\n",
    "                    text=f\"<principles>{principles}</principles>\\n<reason>{reason}</reason>\\n<reward>{reward}</reward>\"\n",
    "                )]\n",
    "            )\n",
    "            conversation.append(model_message)\n",
    "\n",
    "        # Add the test query\n",
    "        user_test_message = types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                types.Part.from_text(\n",
    "                    text=\"Evaluate the following transition.\\n\"\n",
    "                    \"The first image is <image1> (initial), the second is <image2> (result).\\n\"\n",
    "                    \"Return <principles>, <reason>, and a single integer <reward> tag.\"\n",
    "                ),\n",
    "                types.Part.from_bytes(\n",
    "                    data=self.encode_image(test_image1_path),\n",
    "                    mime_type=\"image/jpeg\"\n",
    "                ),\n",
    "                types.Part.from_bytes(\n",
    "                    data=self.encode_image(test_image2_path),\n",
    "                    mime_type=\"image/jpeg\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        conversation.append(user_test_message)\n",
    "\n",
    "        return conversation\n",
    "    \n",
    "    def save_debug_info(self, example_id: str, examples: List[Dict[str, Any]], system_instruction: str,\n",
    "                        test_image1_path: str, test_image2_path: str, response_text: str, subtask: str,\n",
    "                        predicted_reward: int = None, actual_reward: int = None, actual_raw_reward: float = None):\n",
    "        \"\"\"\n",
    "        Save debug information for an example.\n",
    "        \n",
    "        Args:\n",
    "            example_id: Unique identifier for the example\n",
    "            examples: List of examples used for in-context learning\n",
    "            system_instruction: System instruction used\n",
    "            test_image1_path: Path to the first test image\n",
    "            test_image2_path: Path to the second test image\n",
    "            response_text: Response from the model\n",
    "            subtask: Subtask description\n",
    "            predicted_reward: The predicted reward from the model (integer class)\n",
    "            actual_reward: The actual reward (integer class)\n",
    "            actual_raw_reward: The actual raw reward value (float)\n",
    "        \"\"\"\n",
    "        if not self.debug_mode:\n",
    "            return\n",
    "        \n",
    "        # Create a directory for this example\n",
    "        example_dir = os.path.join(self.debug_dir, f\"example_{example_id}\")\n",
    "        if not os.path.exists(example_dir):\n",
    "            os.makedirs(example_dir)\n",
    "        \n",
    "        # Save the input images\n",
    "        input_img_dir = os.path.join(example_dir, \"input_images\")\n",
    "        if not os.path.exists(input_img_dir):\n",
    "            os.makedirs(input_img_dir)\n",
    "        \n",
    "        # Copy test images\n",
    "        test_img1_filename = os.path.basename(test_image1_path)\n",
    "        test_img2_filename = os.path.basename(test_image2_path)\n",
    "        shutil.copy(test_image1_path, os.path.join(input_img_dir, test_img1_filename))\n",
    "        shutil.copy(test_image2_path, os.path.join(input_img_dir, test_img2_filename))\n",
    "        \n",
    "        # Save retrieved example images\n",
    "        example_img_dir = os.path.join(example_dir, \"retrieved_examples\")\n",
    "        if not os.path.exists(example_img_dir):\n",
    "            os.makedirs(example_img_dir)\n",
    "        \n",
    "        for i, ex in enumerate(examples):\n",
    "            ex_img1_filename = f\"ex{i}_img1_{os.path.basename(ex['image1_path'])}\"\n",
    "            ex_img2_filename = f\"ex{i}_img2_{os.path.basename(ex['image2_path'])}\"\n",
    "            shutil.copy(ex['image1_path'], os.path.join(example_img_dir, ex_img1_filename))\n",
    "            shutil.copy(ex['image2_path'], os.path.join(example_img_dir, ex_img2_filename))\n",
    "        \n",
    "        # Save the prompt and response in a text file\n",
    "        with open(os.path.join(example_dir, \"prompt_response.txt\"), \"w\") as f:\n",
    "            f.write(\"===== SUBTASK =====\\n\")\n",
    "            f.write(f\"{subtask}\\n\\n\")\n",
    "            \n",
    "            f.write(\"===== SYSTEM INSTRUCTION =====\\n\")\n",
    "            f.write(f\"{system_instruction}\\n\\n\")\n",
    "            \n",
    "            f.write(\"===== RETRIEVED EXAMPLES =====\\n\")\n",
    "            for i, ex in enumerate(examples):\n",
    "                f.write(f\"Example {i+1}:\\n\")\n",
    "                f.write(f\"  Image1: {ex['image1_path']}\\n\")\n",
    "                f.write(f\"  Image2: {ex['image2_path']}\\n\")\n",
    "                f.write(f\"  Raw Reward: {ex.get('raw_reward', 'N/A')}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"===== TEST IMAGES =====\\n\")\n",
    "            f.write(f\"Test Image1: {test_image1_path}\\n\")\n",
    "            f.write(f\"Test Image2: {test_image2_path}\\n\\n\")\n",
    "            \n",
    "            f.write(\"===== MODEL RESPONSE =====\\n\")\n",
    "            f.write(f\"{response_text}\\n\")\n",
    "            \n",
    "            # Add reward information if available\n",
    "            if predicted_reward is not None or actual_reward is not None:\n",
    "                f.write(\"\\n===== REWARD INFORMATION =====\\n\")\n",
    "                if predicted_reward is not None:\n",
    "                    f.write(f\"Predicted Reward (class): {predicted_reward}\\n\")\n",
    "                if actual_reward is not None:\n",
    "                    f.write(f\"Actual Reward (class): {actual_reward}\\n\")\n",
    "                if actual_raw_reward is not None:\n",
    "                    f.write(f\"Actual Raw Reward: {actual_raw_reward}\\n\")\n",
    "        \n",
    "        # Create a separate rewards summary file for quick reference\n",
    "        with open(os.path.join(example_dir, \"reward_summary.txt\"), \"w\") as f:\n",
    "            f.write(\"===== REWARD SUMMARY =====\\n\")\n",
    "            if predicted_reward is not None:\n",
    "                f.write(f\"Predicted Reward (class): {predicted_reward}\\n\")\n",
    "            if actual_reward is not None:\n",
    "                f.write(f\"Actual Reward (class): {actual_reward}\\n\")\n",
    "            if actual_raw_reward is not None:\n",
    "                f.write(f\"Actual Raw Reward: {actual_raw_reward}\\n\")\n",
    "            \n",
    "            # Add accuracy information\n",
    "            if predicted_reward is not None and actual_reward is not None:\n",
    "                is_correct = predicted_reward == actual_reward\n",
    "                f.write(f\"Prediction Correct: {is_correct}\\n\")\n",
    "                if not is_correct:\n",
    "                    f.write(f\"Error: {predicted_reward - actual_reward}\\n\")\n",
    "        \n",
    "        # Save all information as a JSON file for easier parsing\n",
    "        debug_info = {\n",
    "            \"subtask\": subtask,\n",
    "            \"system_instruction\": system_instruction,\n",
    "            \"retrieved_examples\": [\n",
    "                {\n",
    "                    \"image1_path\": ex['image1_path'],\n",
    "                    \"image2_path\": ex['image2_path'],\n",
    "                    \"raw_reward\": ex.get('raw_reward', None),\n",
    "                    \"copied_image1\": f\"ex{i}_img1_{os.path.basename(ex['image1_path'])}\",\n",
    "                    \"copied_image2\": f\"ex{i}_img2_{os.path.basename(ex['image2_path'])}\"\n",
    "                }\n",
    "                for i, ex in enumerate(examples)\n",
    "            ],\n",
    "            \"test_images\": {\n",
    "                \"image1_path\": test_image1_path,\n",
    "                \"image2_path\": test_image2_path,\n",
    "                \"copied_image1\": test_img1_filename,\n",
    "                \"copied_image2\": test_img2_filename\n",
    "            },\n",
    "            \"model_response\": response_text,\n",
    "            \"rewards\": {\n",
    "                \"predicted_reward\": predicted_reward,\n",
    "                \"actual_reward\": actual_reward,\n",
    "                \"actual_raw_reward\": actual_raw_reward,\n",
    "                \"is_correct\": predicted_reward == actual_reward if predicted_reward is not None and actual_reward is not None else None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(example_dir, \"debug_info.json\"), \"w\") as f:\n",
    "            json.dump(debug_info, f, indent=2)\n",
    "    \n",
    "    def parse_reward_response(self, response_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Parse the reward response from the model.\n",
    "        \n",
    "        Args:\n",
    "            response_text: The text response from the model\n",
    "            \n",
    "        Returns:\n",
    "            Dict with parsed principles, reason, and reward\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            \"principles\": \"\",\n",
    "            \"reason\": \"\",\n",
    "            \"reward\": 0\n",
    "        }\n",
    "        \n",
    "        # Extract principles\n",
    "        principles_match = response_text.find(\"<principles>\")\n",
    "        if principles_match != -1:\n",
    "            principles_end = response_text.find(\"</principles>\", principles_match)\n",
    "            if principles_end != -1:\n",
    "                result[\"principles\"] = response_text[principles_match + 12:principles_end].strip()\n",
    "        \n",
    "        # Extract reason\n",
    "        reason_match = response_text.find(\"<reason>\")\n",
    "        if reason_match != -1:\n",
    "            reason_end = response_text.find(\"</reason>\", reason_match)\n",
    "            if reason_end != -1:\n",
    "                result[\"reason\"] = response_text[reason_match + 8:reason_end].strip()\n",
    "        \n",
    "        # Extract reward\n",
    "        reward_match = response_text.find(\"<reward>\")\n",
    "        if reward_match != -1:\n",
    "            reward_end = response_text.find(\"</reward>\", reward_match)\n",
    "            if reward_end != -1:\n",
    "                try:\n",
    "                    result[\"reward\"] = int(response_text[reward_match + 8:reward_end].strip())\n",
    "                except ValueError:\n",
    "                    # If we can't parse as integer, default to 0\n",
    "                    pass\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_reward(self, image1_path: str, image2_path: str, subtask: str, example_id: str = None, actual_reward_class: int = None, actual_raw_reward: float = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get reward for a pair of images for a given subtask.\n",
    "        \n",
    "        Args:\n",
    "            image1_path: Path to the first image\n",
    "            image2_path: Path to the second image\n",
    "            subtask: Description of the subtask\n",
    "            example_id: Unique identifier for the example (for debug mode)\n",
    "            actual_reward_class: The actual reward class (for debug mode)\n",
    "            actual_raw_reward: The actual raw reward value (for debug mode)\n",
    "            \n",
    "        Returns:\n",
    "            Dict with reward information (principles, reason, reward)\n",
    "        \"\"\"\n",
    "        # If no example_id is provided, generate one\n",
    "        if example_id is None:\n",
    "            example_id = f\"{int(time.time())}_{random.randint(1000, 9999)}\"\n",
    "            \n",
    "        # Retrieve similar examples from memory\n",
    "        examples = self.reward_memory.retrieve(subtask, self.num_examples)\n",
    "        # Create system instruction\n",
    "        system_instruction = self.create_system_instruction(subtask)\n",
    "        \n",
    "        # Create conversation content\n",
    "        conversation = self.create_conversation(\n",
    "            examples, \n",
    "            system_instruction, \n",
    "            image1_path, \n",
    "            image2_path\n",
    "        )\n",
    "        \n",
    "        MAX_TRIES = 3 \n",
    "        curr_try = 0\n",
    "        response = None\n",
    "        \n",
    "        while curr_try < MAX_TRIES:\n",
    "            try:\n",
    "                # Generate the response\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=\"gemini-2.0-flash\",\n",
    "                    contents=conversation,\n",
    "                    config=self.generation_config,\n",
    "                )\n",
    "                break\n",
    "            except errors.APIError as e:\n",
    "                print(f\"Error generating content: {e}\")\n",
    "                time.sleep((2 ** curr_try)*5)  # Exponential backoff\n",
    "                curr_try += 1\n",
    "                if curr_try == MAX_TRIES:\n",
    "                    print(\"Max retries reached. Exiting.\")\n",
    "                    return {\n",
    "                        \"principles\": \"\",\n",
    "                        \"reason\": \"\",\n",
    "                        \"reward\": 0\n",
    "                    }\n",
    "                print(f\"Retrying... ({curr_try}/{MAX_TRIES})\")\n",
    "        \n",
    "        # If we reach here, it means we successfully generated the response\n",
    "        response_text = response.text\n",
    "        \n",
    "        # Parse the response\n",
    "        result = self.parse_reward_response(response_text)\n",
    "        \n",
    "        # Save debug information if in debug mode\n",
    "        if self.debug_mode:\n",
    "            self.save_debug_info(\n",
    "                example_id=example_id,\n",
    "                examples=examples,\n",
    "                system_instruction=system_instruction,\n",
    "                test_image1_path=image1_path,\n",
    "                test_image2_path=image2_path,\n",
    "                response_text=response_text,\n",
    "                subtask=subtask,\n",
    "                predicted_reward=result[\"reward\"],\n",
    "                actual_reward=actual_reward_class,\n",
    "                actual_raw_reward=actual_raw_reward\n",
    "            )\n",
    "        \n",
    "        # Add the image paths for reference\n",
    "        result[\"image1_path\"] = image1_path\n",
    "        result[\"image2_path\"] = image2_path\n",
    "        result[\"subtask\"] = subtask\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "class RewardDatasetSplitter:\n",
    "    def __init__(self, json_file_path: str, base_dir: str = \"\", train_ratio: float = 0.8, seed: int = 42):\n",
    "        \"\"\"\n",
    "        Initialize the dataset splitter.\n",
    "        \n",
    "        Args:\n",
    "            json_file_path: Path to the JSON file containing pairs\n",
    "            base_dir: Base directory to prepend to paths if needed\n",
    "            train_ratio: Ratio of data to use for training (0.0-1.0)\n",
    "            seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.base_dir = base_dir\n",
    "        self.train_ratio = train_ratio\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Load data from JSON\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        # Create a flat list of all examples with subtask information\n",
    "        self.all_examples = []\n",
    "        for subtask, examples in self.data.items():\n",
    "            for example in examples:\n",
    "                # Add base directory to paths if needed\n",
    "                if base_dir and not os.path.isabs(example.get(\"image1_path\", \"\")):\n",
    "                    example[\"image1_path\"] = os.path.join(base_dir, os.path.basename(example[\"image1_path\"]))\n",
    "                if base_dir and not os.path.isabs(example.get(\"image2_path\", \"\")):\n",
    "                    example[\"image2_path\"] = os.path.join(base_dir, os.path.basename(example[\"image2_path\"]))\n",
    "                \n",
    "                # Add subtask information\n",
    "                example[\"subtask\"] = subtask\n",
    "                self.all_examples.append(example)\n",
    "        \n",
    "        # Split into train and test sets\n",
    "        self._split_data()\n",
    "    \n",
    "    def _split_data(self):\n",
    "        \"\"\"Split the data into train and test sets.\"\"\"\n",
    "        # Set random seed for reproducibility\n",
    "        random.seed(self.seed)\n",
    "        \n",
    "        # Shuffle the data\n",
    "        indices = list(range(len(self.all_examples)))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        # Calculate split point\n",
    "        split_idx = int(len(indices) * self.train_ratio)\n",
    "        \n",
    "        # Create train and test sets\n",
    "        self.train_indices = indices[:split_idx]\n",
    "        self.test_indices = indices[split_idx:]\n",
    "        \n",
    "        self.train_examples = [self.all_examples[i] for i in self.train_indices]\n",
    "        self.test_examples = [self.all_examples[i] for i in self.test_indices]\n",
    "    \n",
    "    def get_train_iterator(self, batch_size: int = 1) -> Iterator[List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Get an iterator over the training set.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of examples per batch\n",
    "            \n",
    "        Returns:\n",
    "            Iterator over batches of examples\n",
    "        \"\"\"\n",
    "        for i in range(0, len(self.train_examples), batch_size):\n",
    "            yield self.train_examples[i:i + batch_size]\n",
    "    \n",
    "    def get_test_iterator(self, batch_size: int = 1) -> Iterator[List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Get an iterator over the test set.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of examples per batch\n",
    "            \n",
    "        Returns:\n",
    "            Iterator over batches of examples\n",
    "        \"\"\"\n",
    "        for i in range(0, len(self.test_examples), batch_size):\n",
    "            yield self.test_examples[i:i + batch_size]\n",
    "    \n",
    "    def get_train_examples(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get all training examples.\"\"\"\n",
    "        return self.train_examples\n",
    "    \n",
    "    def get_test_examples(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get all test examples.\"\"\"\n",
    "        return self.test_examples\n",
    "\n",
    "\n",
    "def evaluate_rewarder(rewarder: GeneralRewarder, test_examples: List[Dict[str, Any]]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a rewarder on test examples.\n",
    "    \n",
    "    Args:\n",
    "        rewarder: GeneralRewarder instance\n",
    "        test_examples: List of test examples\n",
    "        \n",
    "    Returns:\n",
    "        Dict with evaluation metrics\n",
    "    \"\"\"\n",
    "    true_rewards = []\n",
    "    pred_rewards = []\n",
    "    \n",
    "    for i, example in enumerate(test_examples):\n",
    "        print(f\"Evaluating example {i+1}/{len(test_examples)}\")\n",
    "        \n",
    "        # Get prediction\n",
    "        result = rewarder.get_reward(\n",
    "            example[\"image1_path\"],\n",
    "            example[\"image2_path\"],\n",
    "            example[\"subtask\"],\n",
    "            example_id=f\"test_{i}\"  # Unique ID for debug mode\n",
    "        )\n",
    "        \n",
    "        global bin_edges\n",
    "        raw_reward = example[\"raw_reward\"]\n",
    "        reward_class = 0\n",
    "        for i in range(len(bin_edges) - 1):\n",
    "            if bin_edges[i] <= raw_reward <= bin_edges[i + 1]:\n",
    "                reward_class = i\n",
    "                break\n",
    "        \n",
    "        # Store true and predicted rewards\n",
    "        true_rewards.append(reward_class)\n",
    "        pred_rewards.append(result[\"reward\"])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score([int(r) for r in true_rewards], [int(r) for r in pred_rewards])\n",
    "    mae = mean_absolute_error([int(r) for r in true_rewards], [int(r) for r in pred_rewards])\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"mae\": mae\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63a7adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Set up command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Run the reward model with optional debug mode.')\n",
    "    parser.add_argument('--debug', action='store_true', help='Enable debug mode to save examples and API responses')\n",
    "    parser.add_argument('--debug_dir', type=str, default='debug_output', help='Directory to save debug information')\n",
    "    parser.add_argument('--num_examples', type=int, default=32, help='Number of examples to use for in-context learning')\n",
    "    parser.add_argument('--json_path', type=str, default='reward_memory_data.json', help='Path to JSON data file')\n",
    "    parser.add_argument('--base_dir', type=str, default='/zfsauton2/home/hshah2/SITCOM/reward_data/', help='Base directory for image paths')\n",
    "    parser.add_argument('--memory_path', type=str, default='./memory/reward_memory_state.pkl', help='Path to saved memory state')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    import dotenv\n",
    "    dotenv.load_dotenv()\n",
    "    API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    \n",
    "    # Initialize splitter with 80/20 split\n",
    "    splitter = RewardDatasetSplitter(\n",
    "        json_file_path=args.json_path,\n",
    "        base_dir=args.base_dir,\n",
    "        train_ratio=0.8\n",
    "    )\n",
    "    \n",
    "    print(f\"Training examples: {len(splitter.get_train_examples())}\")\n",
    "    print(f\"Test examples: {len(splitter.get_test_examples())}\")\n",
    "    \n",
    "    # Load memory\n",
    "    memory = RewardMemory.load(args.memory_path, api_key=API_KEY)\n",
    "    \n",
    "    # Initialize the rewarder with debug mode settings\n",
    "    rewarder = GeneralRewarder(\n",
    "        memory, \n",
    "        api_key=API_KEY, \n",
    "        num_examples=args.num_examples,\n",
    "        debug_mode=args.debug,\n",
    "        debug_dir=args.debug_dir\n",
    "    )\n",
    "    \n",
    "    # Test on a single example\n",
    "    test_example = splitter.get_test_examples()[0]\n",
    "    result = rewarder.get_reward(\n",
    "        test_example[\"image1_path\"],\n",
    "        test_example[\"image2_path\"],\n",
    "        test_example[\"subtask\"],\n",
    "        example_id=\"single_test\"\n",
    "    )\n",
    "    \n",
    "    print(\"Test Result:\")\n",
    "    print(f\"Reward: {result['reward']}\")\n",
    "    print(f\"Reason: {result['reason']}\")\n",
    "    print(f\"\")\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    test_subset = splitter.get_test_examples()  # Use a smaller subset to save API calls\n",
    "    metrics = evaluate_rewarder(rewarder, test_subset)\n",
    "    \n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {metrics['mae']:.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
